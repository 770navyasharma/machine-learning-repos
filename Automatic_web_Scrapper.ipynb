{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02cc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to read stop words from files\n",
    "def read_stop_words(files):\n",
    "    stop_words = set()\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            stop_words.update(word.strip().lower() for word in f.readlines())\n",
    "    return stop_words\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "\n",
    "# Function for NLP analysis\n",
    "def nlp_analysis(text):\n",
    "    stop_words_files = [\n",
    "        \"StopWords_Auditor.txt\",\n",
    "        \"StopWords_currencies.txt\",\n",
    "        \"StopWords_DatesandNumbers.txt\",\n",
    "        \"StopWords_Generic.txt\",\n",
    "        \"StopWords_GenericLong.txt\",\n",
    "        \"StopWords_Names.txt\",\n",
    "        \"StopWords_Geographic.txt\"\n",
    "    ]\n",
    "    stop_words = read_stop_words(stop_words_files)\n",
    "\n",
    "    positive_words_file = \"positive-words.txt\"\n",
    "    negative_words_file = \"negative-words.txt\"\n",
    "\n",
    "    with open(positive_words_file, 'r', encoding='utf-8') as f:\n",
    "        positive_words = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "    with open(negative_words_file, 'r', encoding='utf-8') as f:\n",
    "        negative_words = set(word.strip().lower() for word in f.readlines())\n",
    "\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    positive_score = sum(1 for word in cleaned_text if word in positive_words and word not in stop_words)\n",
    "    negative_score = sum(1 for word in cleaned_text if word in negative_words and word not in stop_words)\n",
    "\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_text) + 0.000001)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_words = len(cleaned_text)\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    average_sentence_length = total_words / total_sentences\n",
    "\n",
    "    complex_words = [word for word in cleaned_text if syllable_count(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / total_words * 100\n",
    "\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "    average_words_per_sentence = total_words / total_sentences\n",
    "\n",
    "    complex_word_count = len(complex_words)\n",
    "\n",
    "    syllable_counts = [syllable_count(word) for word in cleaned_text]\n",
    "    syllable_count_per_word = sum(syllable_counts) / len(cleaned_text)\n",
    "\n",
    "    personal_pronouns = sum(1 for word in cleaned_text if word.lower() in ['i', 'we', 'my', 'ours', 'us'])\n",
    "\n",
    "    total_characters = sum(len(word) for word in cleaned_text)\n",
    "    average_word_length = total_characters / total_words\n",
    "\n",
    "    analysis_result = {\n",
    "        \"Positive Score\": positive_score,\n",
    "        \"Negative Score\": negative_score,\n",
    "        \"Polarity Score\": polarity_score,\n",
    "        \"Subjectivity Score\": subjectivity_score,\n",
    "        \"Average Sentence Length\": average_sentence_length,\n",
    "        \"Percentage of Complex Words\": percentage_complex_words,\n",
    "        \"Fog Index\": fog_index,\n",
    "        \"Average Number of Words Per Sentence\": average_words_per_sentence,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Word Count\": total_words,\n",
    "        \"Syllable Count Per Word\": syllable_count_per_word,\n",
    "        \"Personal Pronouns\": personal_pronouns,\n",
    "        \"Average Word Length\": average_word_length\n",
    "    }\n",
    "\n",
    "    return analysis_result\n",
    "\n",
    "def syllable_count(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def process_url_entry(url_id, url):\n",
    "    print(f\"Processing URL ID: {url_id}\")\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            lambda driver: driver.execute_script('return document.readyState') == 'complete'\n",
    "        )\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        extracted_text = soup.get_text(separator='\\n', strip=True)\n",
    "        analysis_result = nlp_analysis(extracted_text)\n",
    "        print(\"NLP Analysis Result:\")\n",
    "        for key, value in analysis_result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        save_to_csv(url_id, url, analysis_result)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def save_to_csv(url_id, url, analysis_result):\n",
    "    new_data = {\n",
    "        'URL_ID': [url_id],\n",
    "        'URL': [url],\n",
    "        'POSITIVE SCORE': [analysis_result['Positive Score']],\n",
    "        'NEGATIVE SCORE': [analysis_result['Negative Score']],\n",
    "        'POLARITY SCORE': [analysis_result['Polarity Score']],\n",
    "        'SUBJECTIVITY SCORE': [analysis_result['Subjectivity Score']],\n",
    "        'AVG SENTENCE LENGTH': [analysis_result['Average Sentence Length']],\n",
    "        'PERCENTAGE OF COMPLEX WORDS': [analysis_result['Percentage of Complex Words']],\n",
    "        'FOG INDEX': [analysis_result['Fog Index']],\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': [analysis_result['Average Number of Words Per Sentence']],\n",
    "        'COMPLEX WORD COUNT': [analysis_result['Complex Word Count']],\n",
    "        'WORD COUNT': [analysis_result['Word Count']],\n",
    "        'SYLLABLE PER WORD': [analysis_result['Syllable Count Per Word']],\n",
    "        'PERSONAL PRONOUNS': [analysis_result['Personal Pronouns']],\n",
    "        'AVG WORD LENGTH': [analysis_result['Average Word Length']]\n",
    "    }\n",
    "\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    csv_file = 'x.csv'\n",
    "    updated_csv_df = create_or_update_csv_file(csv_file, new_df)\n",
    "    print(\"Last few rows of the updated CSV file:\")\n",
    "    print(updated_csv_df.tail())\n",
    "    print(f\"Data has been successfully added to '{csv_file}'\")\n",
    "\n",
    "def create_or_update_csv_file(file_path, new_data):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        updated_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        updated_df = new_data\n",
    "\n",
    "    updated_df.to_csv(file_path, index=False)\n",
    "    return updated_df\n",
    "\n",
    "input_df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    process_url_entry(url_id, url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
